import asyncio
import aiohttp
import aiofiles
import json
import uvicorn
import logging
import time
import os
import numpy as np
import re
from fastapi import FastAPI, HTTPException, Form, UploadFile, File
from sentence_transformers import SentenceTransformer, util, CrossEncoder
from rank_bm25 import BM25Okapi
from voicerecognise import recognize_audio_with_sdk
from yandex_cloud_ml_sdk import YCloudML
from cachetools import TTLCache
from typing import Dict, List, Optional

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

BASE_DIR = "base"
os.makedirs(BASE_DIR, exist_ok=True)
API_URL = "https://dev.back.matrixcrm.ru/api/v1/AI/servicesByFilters"

YANDEX_FOLDER_ID = "b1gnq2v60fut60hs9vfb"
YANDEX_API_KEY = "AQVNw5Kg0jXoaateYQWdSr2k8cbst_y4_WcbvZrW"

logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
search_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
logger.info("–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

data_cache = TTLCache(maxsize=100, ttl=1800)
embeddings_cache = TTLCache(maxsize=100, ttl=1800)
bm25_cache = TTLCache(maxsize=100, ttl=1800)
conversation_history: Dict[str, Dict] = {}

cache_locks = {
    "data": asyncio.Lock(),
    "embeddings": asyncio.Lock(),
    "bm25": asyncio.Lock()
}

app = FastAPI()

def normalize_text(text: str) -> str:
    text = text.lower().strip()
    text = re.sub(r"[^\w\s\d]", "", text)
    return text

def tokenize_text(text: str) -> List[str]:
    stopwords = {
        "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–∫–∞–∫", "—á—Ç–æ", "—ç—Ç–æ", "–Ω–æ",
        "–∞", "–∏–ª–∏", "—É", "–æ", "–∂–µ", "–∑–∞", "–∫", "–∏–∑", "–æ—Ç", "—Ç–∞–∫", "—Ç–æ", "–≤—Å–µ"
    }
    tokens = text.split()
    return [word for word in tokens if word not in stopwords]

def extract_text_fields(record: dict) -> str:
    excluded_keys = {"id", "categoryId", "currencyId", "langId", "employeeId", "employeeDescription"}
    raw_text = " ".join(
        str(value) for key, value in record.items()
        if key not in excluded_keys and value is not None and value != ""
    )
    return normalize_text(raw_text)

async def load_json_data(tenant_id: str) -> List[dict]:
    file_path = os.path.join(BASE_DIR, f"{tenant_id}.json")
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"–§–∞–π–ª –¥–ª—è tenant_id={tenant_id} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
        content = await f.read()
        return json.loads(content).get("data", {}).get("items", [])

async def prepare_data(tenant_id: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –∏–ª–∏ –æ–Ω–∏ —É—Å—Ç–∞—Ä–µ–ª–∏"""
    async with cache_locks["data"]:
        
        if tenant_id in data_cache:
            cached_data = data_cache[tenant_id]
            if time.time() - cached_data["timestamp"] < 3600:
                return

        records = await load_json_data(tenant_id)
        documents = [extract_text_fields(record) for record in records]
        

        loop = asyncio.get_event_loop()
        embeddings, bm25 = await asyncio.gather(
            loop.run_in_executor(
                None, 
                lambda: search_model.encode(documents, convert_to_tensor=True)
            ),
            loop.run_in_executor(
                None,
                lambda: BM25Okapi([tokenize_text(doc) for doc in documents])
            )
        )
        async with cache_locks["embeddings"], cache_locks["bm25"]:
            data_cache[tenant_id] = {
                "records": records,
                "raw_texts": documents,
                "timestamp": time.time() 
            }
            embeddings_cache[tenant_id] = embeddings
            bm25_cache[tenant_id] = bm25

async def update_json_file(mydtoken: str, tenant_id: str):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ñ–∞–π–ª —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    file_path = os.path.join(BASE_DIR, f"{tenant_id}.json")
    need_update = False

    if not os.path.exists(file_path):
        need_update = True
    else:
        file_age = time.time() - os.path.getmtime(file_path)
        if file_age > 2_592_000:  
            need_update = True

    if not need_update:
        logger.info(f"–§–∞–π–ª {file_path} –∞–∫—Ç—É–∞–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
        return

    try:
        async with aiohttp.ClientSession() as session:
            headers = {"Authorization": f"Bearer {mydtoken}"}
            params = {"tenantId": tenant_id, "page": 1}
            all_data = []

            while True:
                async with session.get(API_URL, headers=headers, params=params) as response:
                    response.raise_for_status()
                    data = await response.json()
                    items = data.get("data", {}).get("items", [])

                    if not items:
                        break

                    all_data.extend(items)
                    logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(items)} –∑–∞–ø–∏—Å–µ–π —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {params['page']}.")
                    params["page"] += 1

            async with aiofiles.open(file_path, "w", encoding="utf-8") as json_file:
                await json_file.write(json.dumps(
                    {"data": {"items": all_data}}, 
                    ensure_ascii=False, 
                    indent=4
                ))

        
        async with cache_locks["data"]:
            if tenant_id in data_cache:
                del data_cache[tenant_id]
            if tenant_id in embeddings_cache:
                del embeddings_cache[tenant_id]
            if tenant_id in bm25_cache:
                del bm25_cache[tenant_id]

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.")

async def rerank_with_cross_encoder(tenant_id: str, query: str, candidates: List[int]) -> List[int]:
    cross_inp = [(query, data_cache[tenant_id]["raw_texts"][idx]) for idx in candidates]
    loop = asyncio.get_event_loop()
    cross_scores = await loop.run_in_executor(
        None,
        lambda: cross_encoder.predict(cross_inp)
    )
    sorted_indices = np.argsort(cross_scores)[::-1].tolist()
    return [candidates[i] for i in sorted_indices]

async def generate_yandexgpt_response(context: str, history: List[dict], question: str) -> str:
    """
    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏. 
    1) –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç 10 —É—Å–ª—É–≥, 
    2) –í—ã–±–∏—Ä–∞–µ—Ç –¥–æ 5 —Å–∞–º—ã—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö, –¥–∞—ë—Ç –∫—Ä–∞—Ç–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç —Ü–µ–Ω—ã, —Ñ–∏–ª–∏–∞–ª—ã, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤, 
    3) –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏—ë–º, –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω.
    """
    system_prompt = """–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏. –ü–∏—à–∏ –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞, –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –∫–∞–∫ –µ—Å–ª–∏ –±—ã —Ç—ã –±—ã–ª –∂–∏–≤—ã–º –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–æ–º:
1. –ü—Ä–æ—á–∏—Ç–∞–π 10 —É—Å–ª—É–≥ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∏ —É—á—Ç–∏, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è –≤–æ–ø—Ä–æ—Å–∞–º–∏ –æ–± —É—Å–ª—É–≥–∞—Ö, —Ü–µ–Ω–∞—Ö, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞—Ö.
2. –í—ã–±–µ—Ä–∏ (–¥–æ) 5 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —É—Å–ª—É–≥ –∏–ª–∏ –ø—Ä–æ—Ü–µ–¥—É—Ä. 
   - –î–ª—è –∫–∞–∂–¥–æ–π –Ω–∞–∑–æ–≤–∏ —Ü–µ–Ω—É, —Ñ–∏–ª–∏–∞–ª, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å).
   - –û–±—ä—è—Å–Ω–∏, –ø–æ—á–µ–º—É —ç—Ç–æ –º–æ–∂–µ—Ç –ø–æ–¥–æ–π—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–∫–æ—Ä–æ—Ç–∫–æ, 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
3. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –≤–µ–∂–ª–∏–≤–æ —Å–æ–æ–±—â–∏, —á—Ç–æ –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å.
4. –í –∫–æ–Ω—Ü–µ –º–æ–∂–µ—à—å –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ —É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–•–æ—á–µ—Ç—Å—è –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é?¬ª).
5. –°—Ç–∞—Ä–∞–π—Å—è –ø–∏—Å–∞—Ç—å –∂–∏–≤—ã–º –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º —Å—Ç–∏–ª–µ–º, –∏–∑–±–µ–≥–∞—è –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤.
6.–ü–∏—à–∏ –≤–µ–∂–ª–∏–≤–æ –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ.
- –î–æ–±–∞–≤–ª—è–π –≤ –æ—Ç–≤–µ—Ç—ã —ç–º–æ–¥–∑–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, üòä, üòâ, üëç), –æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–∏ –∏–ª–∏ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö.
- –û—Ä–∏–µ–Ω—Ç–∏—Ä—É–π—Å—è –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ —Ü–µ–Ω—ã, –º–æ–∂–µ—à—å –ø–æ—Å—Ç–∞–≤–∏—Ç—å ¬´üí∏¬ª, –µ—Å–ª–∏ –ø—Ä–æ –∑–∞–ø–∏—Å—å ‚Äî ¬´üóì¬ª, –∏ —Ç. –ø.
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –≤–µ–∂–ª–∏–≤–æ —Å–æ–æ–±—â–∏ –æ–± —ç—Ç–æ–º –±–µ–∑ —ç–º–æ–¥–∑–∏.
7. –£–∫–∞–∑—ã–≤–∞–π –≤—Å–µ —Ü–µ–Ω—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –Ω–µ –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–æ–º,–∞ –¥–æ–±–∞–≤–ª—è–π –ø—Ä–µ–¥–ª–æ–≥ –æ—Ç,–Ω–∞–ø—Ä–∏–º–µ—Ä –æ—Ç 12000 —Ä—É–±–ª–µ–π,–∏ —Ç–∞–∫ –¥–∞–ª–µ–µ
"""

    messages = [
        {"role": "system", "text": system_prompt},
        {"role": "system", "text": f"–í–æ—Ç —Å–ø–∏—Å–æ–∫ 10 —É—Å–ª—É–≥:\n{context}\n"}
    ]
    
    for entry in history[-3:]:
        messages.append({"role": "user", "text": entry['user_query']})
        messages.append({"role": "assistant", "text": entry['assistant_response']})

    messages.append({"role": "user", "text": question})

    try:
        loop = asyncio.get_event_loop()
        sdk = YCloudML(folder_id=YANDEX_FOLDER_ID, auth=YANDEX_API_KEY)
        model_uri = f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-32k/rc"

        result = await loop.run_in_executor(
            None,
            lambda: sdk.models.completions(model_uri)
                .configure(temperature=0.7, max_tokens=2000)
                .run(messages)
        )

        if result and result.alternatives:
            return result.alternatives[0].text.strip()
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –º–Ω–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ YandexGPT API: {str(e)}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

@app.post("/ask")
async def ask_assistant(
    user_id: str = Form(...),
    question: Optional[str] = Form(None),
    mydtoken: str = Form(...),
    tenant_id: str = Form(...),
    file: UploadFile = File(None)
):
    try:
        current_time = time.time()
        expired_users = [
            uid for uid, data in conversation_history.items()
            if current_time - data["last_active"] > 1800
        ]
        for uid in expired_users:
            del conversation_history[uid]

        recognized_text = None

        if file and file.filename:
            temp_path = f"/tmp/{file.filename}"
            try:
                async with aiofiles.open(temp_path, "wb") as temp_file:
                    await temp_file.write(await file.read())
                
                loop = asyncio.get_event_loop()
                recognized_text = await loop.run_in_executor(
                    None, 
                    lambda: recognize_audio_with_sdk(temp_path)
                )
            finally:
                if os.path.exists(temp_path):
                    os.remove(temp_path)

            if not recognized_text:
                raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ —Ñ–∞–π–ª–∞.")

        input_text = recognized_text or question
        if not input_text:
            raise HTTPException(status_code=400, detail="–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª.")

        force_update = False 
        
        if force_update or tenant_id not in data_cache:
            await update_json_file(mydtoken, tenant_id)
        
        await prepare_data(tenant_id)

        normalized_question = normalize_text(input_text)
        tokenized_query = tokenize_text(normalized_question)

        bm25_scores = bm25_cache[tenant_id].get_scores(tokenized_query)
        top_bm25_indices = np.argsort(bm25_scores)[::-1][:30].tolist()

        loop = asyncio.get_event_loop()
        query_embedding = await loop.run_in_executor(
            None,
            lambda: search_model.encode(normalized_question, convert_to_tensor=True)
        )
        similarities = util.pytorch_cos_sim(query_embedding, embeddings_cache[tenant_id])
        top_vector_indices = similarities[0].topk(30).indices.tolist()

        combined_indices = list(set(top_bm25_indices + top_vector_indices))[:50]

        reranked_indices = await rerank_with_cross_encoder(tenant_id, normalized_question, combined_indices)
        top_10_indices = reranked_indices[:10]

        context = "\n".join([
            f"{i+1}. –£—Å–ª—É–≥–∞: {data_cache[tenant_id]['records'][idx].get('serviceName', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n"
            f"   –¶–µ–Ω–∞: {data_cache[tenant_id]['records'][idx].get('price', '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞')} —Ä—É–±.\n"
            f"   –§–∏–ª–∏–∞–ª: {data_cache[tenant_id]['records'][idx].get('filialName', '–§–∏–ª–∏–∞–ª –Ω–µ —É–∫–∞–∑–∞–Ω')}\n"
            f"   –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: {data_cache[tenant_id]['records'][idx].get('employeeFullName', '–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –Ω–µ —É–∫–∞–∑–∞–Ω')}"
            for i, idx in enumerate(top_10_indices)
        ])

        if user_id not in conversation_history:
            conversation_history[user_id] = {
                "history": [],
                "last_active": time.time(),
                "greeted": False
            }

        if not conversation_history[user_id]["greeted"]:
            context = (
                "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏. "
                "–° —Ä–∞–¥–æ—Å—Ç—å—é –ø–æ–º–æ–≥—É –≤–∞–º –ø–æ–¥–æ–±—Ä–∞—Ç—å —É—Å–ª—É–≥—É –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏—ë–º.\n"
                + context
            )
            conversation_history[user_id]["greeted"] = True

        conversation_history[user_id]["last_active"] = time.time()

        response_text = await generate_yandexgpt_response(
            context=context,
            history=conversation_history[user_id]["history"],
            question=input_text
        )

        conversation_history[user_id]["history"].append({
            "user_query": input_text,
            "assistant_response": response_text,
            "search_results": [data_cache[tenant_id]['records'][idx] for idx in top_10_indices]
        })

        return {"response": response_text}

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")

if __name__ == "__main__":
    logger.info("–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É 8001...")
    uvicorn.run(app, host="0.0.0.0", port=8001)
