import asyncio
import aiohttp
import aiofiles
import json
import uvicorn
import logging
import time
import os
import numpy as np
import re
import pickle
from pathlib import Path
from fastapi import FastAPI, HTTPException, Form, UploadFile, File
from sentence_transformers import SentenceTransformer,CrossEncoder
from rank_bm25 import BM25Okapi
from voicerecognise import recognize_audio_with_sdk
from yandex_cloud_ml_sdk import YCloudML
from typing import Dict, List, Optional
import faiss  

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)


BASE_DIR = "base"
EMBEDDINGS_DIR = "embeddings_data"
os.makedirs(BASE_DIR, exist_ok=True)
os.makedirs(EMBEDDINGS_DIR, exist_ok=True)
API_URL = "https://dev.back.matrixcrm.ru/api/v1/AI/servicesByFilters"

YANDEX_FOLDER_ID = "b1gnq2v60fut60hs9vfb"
YANDEX_API_KEY = "AQVNw5Kg0jXoaateYQWdSr2k8cbst_y4_WcbvZrW"


logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
search_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet-base-v2")
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')  # –ö—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
logger.info("–ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

conversation_history: Dict[str, Dict] = {}

app = FastAPI()

def get_tenant_path(tenant_id: str) -> Path:
    """–°–æ–∑–¥–∞–µ—Ç –ø–∞–ø–∫—É –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–µ–Ω–∞–Ω—Ç–∞"""
    tenant_path = Path(EMBEDDINGS_DIR) / tenant_id
    tenant_path.mkdir(parents=True, exist_ok=True)
    return tenant_path

def normalize_text(text: str) -> str:
    text = text.lower().strip()
    text = re.sub(r"[^\w\s\d]", "", text)
    return text

def tokenize_text(text: str) -> List[str]:
    stopwords = {
        "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–∫–∞–∫", "—á—Ç–æ", "—ç—Ç–æ", "–Ω–æ",
        "–∞", "–∏–ª–∏", "—É", "–æ", "–∂–µ", "–∑–∞", "–∫", "–∏–∑", "–æ—Ç", "—Ç–∞–∫", "—Ç–æ", "–≤—Å–µ"
    }
    tokens = text.split()
    return [word for word in tokens if word not in stopwords]

def extract_text_fields(record: dict) -> str:
    excluded_keys = {"id", "categoryId", "currencyId", "langId", "employeeId", "employeeDescription"}
    raw_text = " ".join(
        str(value) for key, value in record.items()
        if key not in excluded_keys and value is not None and value != ""
    )
    return normalize_text(raw_text)

async def load_json_data(tenant_id: str) -> List[dict]:
    file_path = os.path.join(BASE_DIR, f"{tenant_id}.json")
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"–§–∞–π–ª –¥–ª—è tenant_id={tenant_id} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
        content = await f.read()
        return json.loads(content).get("data", {}).get("items", [])

async def prepare_data(tenant_id: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ–Ω–∞–Ω—Ç–∞"""
    tenant_path = get_tenant_path(tenant_id)
    data_file = tenant_path / "data.json"
    embeddings_file = tenant_path / "embeddings.npy"
    bm25_file = tenant_path / "bm25.pkl"
    faiss_index_file = tenant_path / "faiss_index.index"  

    if all([f.exists() for f in [data_file, embeddings_file, bm25_file, faiss_index_file]]):
        file_age = time.time() - os.path.getmtime(data_file)
        if file_age < 2_592_000: 
            async with aiofiles.open(data_file, "r") as f:
                data = json.loads(await f.read())
            
            embeddings = np.load(embeddings_file)
            with open(bm25_file, "rb") as f:
                bm25 = pickle.load(f)
            
            index = faiss.read_index(str(faiss_index_file))  
            return data, embeddings, bm25, index

    records = await load_json_data(tenant_id)
    documents = [extract_text_fields(record) for record in records]

    loop = asyncio.get_event_loop()
    embeddings, bm25 = await asyncio.gather(
        loop.run_in_executor(
            None, 
            lambda: search_model.encode(documents, convert_to_tensor=True).cpu().numpy()
        ),
        loop.run_in_executor(
            None,
            lambda: BM25Okapi([tokenize_text(doc) for doc in documents])
        )
    )

   
    dimension = embeddings.shape[1]  
    index = faiss.IndexFlatL2(dimension)  
    index.add(embeddings)  
    faiss.write_index(index, str(faiss_index_file)) 

    async with aiofiles.open(data_file, "w") as f:
        await f.write(json.dumps({
            "records": records,
            "raw_texts": documents,
            "timestamp": time.time()
        }))

    np.save(embeddings_file, embeddings)
    with open(bm25_file, "wb") as f:
        pickle.dump(bm25, f)

    return {"records": records, "raw_texts": documents}, embeddings, bm25, index

async def update_json_file(mydtoken: str, tenant_id: str):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ —É–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã"""
    tenant_path = get_tenant_path(tenant_id)
    file_path = os.path.join(BASE_DIR, f"{tenant_id}.json")

    if os.path.exists(file_path):
        file_age = time.time() - os.path.getmtime(file_path)
        if file_age < 2_592_000:
            logger.info(f"–§–∞–π–ª {file_path} –∞–∫—Ç—É–∞–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
            return

    for f in tenant_path.glob("*"):
        try:
            os.remove(f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {f}: {e}")

    try:
        async with aiohttp.ClientSession() as session:
            headers = {"Authorization": f"Bearer {mydtoken}"}
            params = {"tenantId": tenant_id, "page": 1}
            all_data = []
            while True:
                async with session.get(API_URL, headers=headers, params=params) as response:
                    response.raise_for_status()
                    data = await response.json()
                    items = data.get("data", {}).get("items", [])

                    if not items:
                        break

                    all_data.extend(items)
                    logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(items)} –∑–∞–ø–∏—Å–µ–π —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {params['page']}.")
                    params["page"] += 1

            async with aiofiles.open(file_path, "w", encoding="utf-8") as json_file:
                await json_file.write(json.dumps(
                    {"data": {"items": all_data}}, 
                    ensure_ascii=False, 
                    indent=4
                ))

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.")

async def rerank_with_cross_encoder(query: str, candidates: List[int], raw_texts: List[str]) -> List[int]:
    """–†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —Ç–æ–ø-10 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä–∞"""
    cross_inp = [(query, raw_texts[idx]) for idx in candidates]
    loop = asyncio.get_event_loop()
    cross_scores = await loop.run_in_executor(
        None,
        lambda: cross_encoder.predict(cross_inp)
    )
    sorted_indices = np.argsort(cross_scores)[::-1].tolist()
    return [candidates[i] for i in sorted_indices]

async def generate_yandexgpt_response(context: str, history: List[dict], question: str) -> str:
    """
    –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏. 
    1) –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç 10 —É—Å–ª—É–≥, 
    2) –í—ã–±–∏—Ä–∞–µ—Ç –¥–æ 5 —Å–∞–º—ã—Ö –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö, –¥–∞—ë—Ç –∫—Ä–∞—Ç–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç —Ü–µ–Ω—ã, —Ñ–∏–ª–∏–∞–ª—ã, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤, 
    3) –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏—ë–º, –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ç–æ–Ω.
    """
    system_prompt = """# üîπ **–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏** üîπ  
**–¢—ã ‚Äì –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–ª–∏–Ω–∏–∫–∏ MED YU MED.** –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äì –ø–æ–º–æ–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –Ω–∞—Ö–æ–¥–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞—Ö, —É—Å–ª—É–≥–∞—Ö, —Ñ–∏–ª–∏–∞–ª–∞—Ö –∏ —Ü–µ–Ω–∞—Ö. 
–¢—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –∫–∞–∫ **RAG-–º–æ–¥–µ–ª—å (Retrieval-Augmented Generation)**, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ:  
1. **–í–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω** ‚Äì –≤ –Ω—ë–º –µ—Å—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞—Ö, —É—Å–ª—É–≥–∞—Ö, —Ü–µ–Ω–∞—Ö –∏ —Ñ–∏–ª–∏–∞–ª–∞—Ö. 
2. **–¢–µ–±–µ –Ω–µ –Ω—É–∂–Ω–æ –≤—ã–¥—É–º—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é** ‚Äì –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –ø—Ä—è–º–æ —Å–æ–æ–±—â–∞–π –æ–± —ç—Ç–æ–º. 
3. **–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É—Ç–æ—á–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å**, —Ç—ã –æ–±—è–∑–∞–Ω –∏—Å–∫–∞—Ç—å –æ—Ç–≤–µ—Ç –∏–º–µ–Ω–Ω–æ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –≤ –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞. 
## üìå **1. –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞**  
- –ü—Ä–æ—á–∏—Ç–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø–æ–π–º–∏, –∫ —á–µ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∑–∞–ø—Ä–æ—Å: **—É—Å–ª—É–≥–∏, —Ü–µ–Ω—ã, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã, —Ñ–∏–ª–∏–∞–ª–∞ –∏ —Ç. –¥.**  
- –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –Ω–µ—è—Å–µ–Ω ‚Äì **–∑–∞–¥–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å** –≤–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –¥–æ–≥–∞–¥—ã–≤–∞—Ç—å—Å—è. 
## üîç **2. –ü–æ–¥–±–æ—Ä —É—Å–ª—É–≥**  
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ —É—Å–ª—É–≥–∏, —Ç—ã –æ–±—è–∑–∞–Ω:  
- –ù–∞–π—Ç–∏ **–≤—Å–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —É—Å–ª—É–≥–∏**. 
- –£–∫–∞–∑–∞—Ç—å **—Ü–µ–Ω—É** (–Ω–∞–ø—Ä–∏–º–µ—Ä, `"–æ—Ç 12000 —Ä—É–±–ª–µ–π üí∏"`). 
- –ù–∞–∑–≤–∞—Ç—å **—Ñ–∏–ª–∏–∞–ª**, –≥–¥–µ –¥–æ—Å—Ç—É–ø–Ω–∞ —É—Å–ª—É–≥–∞ (**–ú–æ—Å–∫–≤–∞ ‚Äì –•–æ–¥—ã–Ω–∫–∞, –ú–æ—Å–∫–≤–∞ ‚Äì –°–∏—Ç–∏, –î—É–±–∞–π**). 
- –ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç—å **–≤—Å–µ—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤**, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç —ç—Ç—É —É—Å–ª—É–≥—É (**–±–µ–∑ —Å–ª–æ–≤ "–∏ –¥—Ä—É–≥–∏–µ", —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã–µ —Å–ø–∏—Å–∫–∏!**). 
- –û–±—ä—è—Å–Ω–∏—Ç—å **–ø–æ–ª—å–∑—É —É—Å–ª—É–≥–∏** –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä: `"–≠—Ç–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –ø–æ–º–æ–∂–µ—Ç —É–±—Ä–∞—Ç—å –º–æ—Ä—â–∏–Ω—ã –∏ —Å–¥–µ–ª–∞—Ç—å –∫–æ–∂—É –±–æ–ª–µ–µ —É–ø—Ä—É–≥–æ–π ‚ú®"`). 
## üìç **3. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ**  
- –ß–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏, —á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å. 
- **–ü—Ä–∏–º–µ—Ä:** `"–ü–æ–∫–∞ –Ω–µ –Ω–∞—à–ª–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–æ –º–æ–≥—É –ø–æ–º–æ—á—å, –µ—Å–ª–∏ —É—Ç–æ—á–Ω–∏—à—å –¥–µ—Ç–∞–ª–∏ üòä"`. 
## üéØ **4. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞**  
- –ï—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ, **–ø—Ä–µ–¥–ª–æ–∂–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è** (`"–•–æ—á–µ—à—å –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ —É–¥–æ–±–Ω–æ–µ –≤—Ä–µ–º—è? üóì"`). 
- –ó–∞–∫–∞–Ω—á–∏–≤–∞–π –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –Ω–æ **–Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π –æ—Ç–≤–µ—Ç —ç–º–æ–¥–∑–∏**. 
## üí¨ **5. –ö–∞–∫ –≤–µ—Å—Ç–∏ –¥–∏–∞–ª–æ–≥**  
- **–ü–∏—à–∏ –∂–∏–≤–æ –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ**, –∏–∑–±–µ–≥–∞—è –∫–∞–Ω—Ü–µ–ª—è—Ä–∏–∑–º–æ–≤. 
- –ò—Å–ø–æ–ª—å–∑—É–π **—ç–º–æ–¥–∑–∏ —É–º–µ—Ä–µ–Ω–Ω–æ**, –ø–æ —Å–º—ã—Å–ª—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, `"üí∏"` –¥–ª—è —Ü–µ–Ω, `"üóì"` –¥–ª—è –∑–∞–ø–∏—Å–∏). 
- **–£—á–∏—Ç—ã–≤–∞–π –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞** ‚Äì –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É—Ç–æ—á–Ω—è–µ—Ç –¥–µ—Ç–∞–ª–∏, —Ç—ã –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –æ—Ç–≤–µ—Ç—ã. 
- **–ü—Ä–æ—è–≤–ª—è–π —ç–º–ø–∞—Ç–∏—é**: –µ—Å–ª–∏ —á–µ–ª–æ–≤–µ–∫ –¥–µ–ª–∏—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–æ–π, –ø–æ–∫–∞–∂–∏, —á—Ç–æ –ø–æ–Ω–∏–º–∞–µ—à—å –µ–≥–æ —Å–∏—Ç—É–∞—Ü–∏—é. 
## üö® **6. –û—Å–æ–±—ã–µ —É–∫–∞–∑–∞–Ω–∏—è**  
- –í –∫–ª–∏–Ω–∏–∫–µ —Ç—Ä–∏ —Ñ–∏–ª–∏–∞–ª–∞: **–ú–æ—Å–∫–≤–∞ (–•–æ–¥—ã–Ω–∫–∞, –°–∏—Ç–∏), –î—É–±–∞–π**(Bluewaters).
- –î–µ—Ä–∂–∏ –≤–µ—Å—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø–∞–º—è—Ç–∏,—Ç–∞–∫ –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –∑–∞–¥–∞–≤–∞—Ç—å –º–Ω–æ–≥–æ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
- –ï—Å–ª–∏ —É—Å–ª—É–≥–∞/—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ñ–∏–ª–∏–∞–ª—É, **–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π —ç—Ç–æ**. 
- **–ù–µ –ø–∏—à–∏ "–î–æ–±—Ä—ã–π –¥–µ–Ω—å" –ø–æ–≤—Ç–æ—Ä–Ω–æ –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞**. 
- **–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø—Ä–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞**, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ **–∏—â–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ**, –¥–∞–∂–µ –µ—Å–ª–∏ —Ä–∞–Ω–µ–µ –¥–∞–≤–∞–ª –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç. 
## ‚ö†Ô∏è **7. –ì–ª–∞–≤–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ**  
–¢—ã ‚Äì **RAG-–º–æ–¥–µ–ª—å**, –∏ –≤—Å—ë, —á—Ç–æ —Ç–µ–±–µ –Ω—É–∂–Ω–æ, —É–∂–µ –µ—Å—Ç—å –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. 
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç, **–Ω–µ –≤—ã–¥—É–º—ã–≤–∞–π** ‚Äì –ª—É—á—à–µ —Å–ø—Ä–æ—Å–∏ —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å. 
"""
    messages = [
        {"role": "system", "text": system_prompt},
        {"role": "system", "text": f"–í–æ—Ç —Å–ø–∏—Å–æ–∫ 10 —É—Å–ª—É–≥:\n{context}\n"}
    ]
    
    for entry in history[-10:]:
        messages.append({"role": "user", "text": entry['user_query']})
        messages.append({"role": "assistant", "text": entry['assistant_response']})

    messages.append({"role": "user", "text": question})

    try:
        loop = asyncio.get_event_loop()
        sdk = YCloudML(folder_id=YANDEX_FOLDER_ID, auth=YANDEX_API_KEY)
        model_uri = f"gpt://{YANDEX_FOLDER_ID}/yandexgpt-32k/rc"

        result = await loop.run_in_executor(
            None,
            lambda: sdk.models.completions(model_uri)
                .configure(temperature=0.6, max_tokens=4096)
                .run(messages)
        )

        if result and result.alternatives:
            return result.alternatives[0].text.strip()
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –º–Ω–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ YandexGPT API: {str(e)}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

@app.post("/ask")
async def ask_assistant(
    user_id: str = Form(...),
    question: Optional[str] = Form(None),
    mydtoken: str = Form(...),
    tenant_id: str = Form(...),
    file: UploadFile = File(None)
):
    try:
        current_time = time.time()
        expired_users = [
            uid for uid, data in conversation_history.items()
            if current_time - data["last_active"] > 1800
        ]
        for uid in expired_users:
            del conversation_history[uid]

        recognized_text = None

        if file and file.filename:
            temp_path = f"/tmp/{file.filename}"
            try:
                async with aiofiles.open(temp_path, "wb") as temp_file:
                    await temp_file.write(await file.read())
                
                loop = asyncio.get_event_loop()
                recognized_text = await loop.run_in_executor(
                    None, 
                    lambda: recognize_audio_with_sdk(temp_path)
                )
            finally:
                if os.path.exists(temp_path):
                    os.remove(temp_path)

            if not recognized_text:
                raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏–∑ —Ñ–∞–π–ª–∞.")

        input_text = recognized_text or question
        if not input_text:
            raise HTTPException(status_code=400, detail="–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª.")

        force_update = False 
        
        if force_update or not (get_tenant_path(tenant_id) / "data.json").exists():
            await update_json_file(mydtoken, tenant_id)
        
        data_dict, embeddings, bm25, faiss_index = await prepare_data(tenant_id)

        normalized_question = normalize_text(input_text)
        tokenized_query = tokenize_text(normalized_question)

        bm25_scores = bm25.get_scores(tokenized_query)
        top_bm25_indices = np.argsort(bm25_scores)[::-1][:50].tolist()

        
        loop = asyncio.get_event_loop()
        query_embedding = await loop.run_in_executor(
            None,
            lambda: search_model.encode(normalized_question, convert_to_tensor=True).cpu().numpy()
        )
        D, I = faiss_index.search(query_embedding.reshape(1, -1), 50) 
        top_faiss_indices = I[0].tolist()

        
        combined_indices = list(set(top_bm25_indices + top_faiss_indices))[:50]

       
        top_10_indices = await rerank_with_cross_encoder(
            query=normalized_question,
            candidates=combined_indices[:10],
            raw_texts=data_dict["raw_texts"]
        )

        
        context = "\n".join([
            f"{i+1}. –£—Å–ª—É–≥–∞: {data_dict['records'][idx].get('serviceName', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}\n"
            f"   –¶–µ–Ω–∞: {data_dict['records'][idx].get('price', '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞')} —Ä—É–±.\n"
            f"   –§–∏–ª–∏–∞–ª: {data_dict['records'][idx].get('filialName', '–§–∏–ª–∏–∞–ª –Ω–µ —É–∫–∞–∑–∞–Ω')}\n"
            f"   –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: {data_dict['records'][idx].get('employeeFullName', '–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –Ω–µ —É–∫–∞–∑–∞–Ω')}"
            for i, idx in enumerate(top_10_indices)
        ])

        if user_id not in conversation_history:
            conversation_history[user_id] = {
                "history": [],
                "last_active": time.time(),
                "greeted": False
            }

        if not conversation_history[user_id]["greeted"]:
            context = (
                "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–ª–∏–Ω–∏–∫–∏. "
                "–° —Ä–∞–¥–æ—Å—Ç—å—é –ø–æ–º–æ–≥—É –≤–∞–º –ø–æ–¥–æ–±—Ä–∞—Ç—å —É—Å–ª—É–≥—É –∏–ª–∏ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–∏—ë–º.\n"
                + context
            )
            conversation_history[user_id]["greeted"] = True

        conversation_history[user_id]["last_active"] = time.time()

        response_text = await generate_yandexgpt_response(
            context=context,
            history=conversation_history[user_id]["history"],
            question=input_text
        )

        conversation_history[user_id]["history"].append({
            "user_query": input_text,
            "assistant_response": response_text,
            "search_results": [data_dict['records'][idx] for idx in top_10_indices]
        })

        return {"response": response_text}

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")

if __name__ == "__main__":
    logger.info("–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–∞ –ø–æ—Ä—Ç—É 8001...")
    uvicorn.run(app, host="0.0.0.0", port=8001)
